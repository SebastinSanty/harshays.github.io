<!doctype html>
<html lang="en">
  <head>
    <!-- meta  -->
    <meta type="author" content="Harshay Shah"/>
    <meta charset="utf-8" http-eqiv="content-type" content="text/html; charset=utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0, shrink-to-fit=no">

    <!-- cdn + font -->
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css?family=Gentium+Basic:400,700&display=swap" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script>

    <!-- css/js -->
    <link rel="stylesheet" type="text/css" href="./css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="./css/academicons.min.css">
    <link rel="stylesheet" type="text/css" href="./css/all.min.css">
    <link rel="stylesheet" type="text/css" href="./css/style.css">

    <script src="./js/jquery-2.2.4.min.js"></script>
    <script src="./js/app.js"></script>

    <title>Harshay Shah</title>
  </head>

  <body>
    <div class="container">

      <!-- Header -->
      <div class="row header-row">
              <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>
              <div class="col-lg-8 col-md-10 col-sm-12 header-column">
                <div class="row">

                  <div class="col-xs-4 img-xs text-center hidden-sm hidden-md hidden-lg">
                    <img src="resources/me.jpg" class="img-rounded img-responsive">
                  </div>

                  <!-- <div class="col-sm-12 col-xs-8 hidden-sm hidden-md hidden-lg"> -->
                  <div class="col-sm-12 col-xs-8">
                    <div class="row">
                      <div class="col-sm-6 hidden-sm hidden-md hidden-lg">
                          <p class="header-xs">Harshay <span class="last-name">Shah</span></p>
                      </div>

                      <div class="col-sm-6 header-right">
                        <div class="social-links-xs text-left hidden-sm hidden-md hidden-lg">
                          <a class="icons" href="./resources/cv.pdf" target="_blank"><i class="ai ai-cv-square ai-1x"></i></a>
                          <a class="icons" href="mailto:harshay.rshah@gmail.com" target="_blank"><i class="fas fa-envelope-square fa-1x"></i></a>
                          <a class="icons" href="https://twitter.com/harshays_" target="_blank"><i class="fab fa-twitter-square fa-1x"></i></a>
                          <a class="icons" href="https://github.com/harshays" target="_blank"><i class="fab fa-github-square fa-1x"></i></a>
                          <a class="icons" href="https://scholar.google.com/citations?user=oC8YKjUAAAAJ" target="_blank"><i class="ai ai-google-scholar-square ai-1x"></i></a>
                          <!-- <a class="icons" href="https://linkedin.com/in/harshayshah" target="_blank"><i class="fab fa-linkedin-in fa-1x"></i></a> -->
                          <!-- <a href="./blog"><i class="fas fa-pen-square fa-3x"></i></a> -->
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
              <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>
      </div>

      <!-- Bio etc -->
      <div class="content-row row">
        <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>
        <div class="col-lg-8 col-md-10 col-xs-12">
          <div class="content-inner-row row">

            <div class="col-sm-3 hidden-xs img-col text-center">
              <div class="img-div">
                <img src="resources/me.jpg" alt="my face" class="img-md hidden-xs img-rounded img-responsive">
              </div>

                <div class="social-links text-center">
                  <a class="icons" href="./resources/cv.pdf" target="_blank"><i class="ai ai-cv-square ai-3x"></i></a>
                  <a class="icons" href="mailto:harshay.rshah@gmail.com" target="_blank"><i class="fas fa-envelope-square fa-3x"></i></a>
                  <a class="icons" href="https://github.com/harshays" target="_blank"><i class="fab fa-github-square fa-3x"></i></a>
                  <a class="icons" href="https://twitter.com/harshays_" target="_blank"><i class="fab fa-twitter-square fa-3x"></i></a>
                  <a href="https://scholar.google.com/citations?user=oC8YKjUAAAAJ" target="_blank"><i class="ai ai-google-scholar-square ai-3x"></i></a>
                  <!-- <a class="icons" href="https://linkedin.com/in/harshayshah" target="_blank"><i class="fab fa-linkedin fa-3x"></i></a> -->
                  <!-- <a href="./blog"><i class="fas fa-pen-square fa-3x"></i></a> -->
                </div>
            </div>

            <div class="col-xs-12 col-sm-9 content-col">
              <p class="header hidden-xs">Harshay <span class="last-name">Shah</span></p>
              <!-- <p>
                I am a <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-india/">research fellow</a> at
                <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-india/">Microsoft Research India</a>,
                where I work with <a href="http://praneethnetrapalli.org/">Praneeth Netrapalli</a>
                and <a href="http://www.prateekjain.org/">Prateek Jain</a>.
                I am broadly interested in understanding deep learning phenomena
                and making machine learning more reliable. 
              </p>
              <p>
                I received my BS in Computer Science and Statistics from the 
                <a href="https://cs.illinois.edu/">University of Illinois at Urbana-Champaign (UIUC)</a>,
                where I was advised by <a href="http://sundaram.cs.illinois.edu/">Hari Sundaram</a>,
                <a href="https://homes.cs.washington.edu/~sewoong/">Sewoong Oh</a>, and
                <a href="http://sanmi.cs.illinois.edu/">Sanmi Koyejo</a>.
                In my free time, I like to read, play tennis, and watch cricket.
              </p> -->
            </div>

            
          </div>
          <hr>
        </div>
        <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>
      </div>

      <div id="about" class="paper-row row">
        <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>
        <div class="col-lg-8 col-md-10 col-sm-12">
          <div class="subheader-row row">
            <div class="col-xs-12 subheader-col">
              <p class="subheader">About</p>
            </div>
          </div>

          <div class="papers-content-row row">
            <div class="col-xs-12 content-col">

              <p>
                I am a <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-india/">research fellow</a> at
                <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-india/">Microsoft Research India</a>,
                where I work with <a href="http://praneethnetrapalli.org/">Praneeth Netrapalli</a>
                and <a href="http://www.prateekjain.org/">Prateek Jain</a>.
                I am broadly interested in understanding deep learning phenomena
                and making machine learning more reliable. 
              </p>
              <p>
                I received my BS in Computer Science and Statistics from the 
                <a href="https://cs.illinois.edu/">University of Illinois at Urbana-Champaign (UIUC)</a>,
                where I was advised by <a href="http://sundaram.cs.illinois.edu/">Hari Sundaram</a>,
                <a href="https://homes.cs.washington.edu/~sewoong/">Sewoong Oh</a>, and
                <a href="http://sanmi.cs.illinois.edu/">Sanmi Koyejo</a>.
                In my free time, I like to read, play tennis, and watch cricket.
              </p>

              <!-- Input Gradients -->
              <!-- <div class="paper-div">
                <a class="paper-title" target="_blank" href="https://arxiv.org/abs/2102.12781">Do Input Gradients Highlight Discriminative Features?</a>

                <p class="paper-info">
                  <span class="paper-authors"><span class="underline">Harshay Shah</span>, Prateek Jain, and Praneeth Netrapalli</span><br/>
                  <span class="paper-venue">ICLR workshop on Science and Engineering of Deep Learning</span>
                  <span class="paper-venue-abbrv">(<a target="_blank" href="https://sites.google.com/view/sedl-workshop">SEDL</a>)</span>, 2021
                  <img class="shields" src="https://img.shields.io/badge/-oral-darkred?style=flat-square" alt=""><br>
                  <span class="paper-venue">ICLR workshop on Responsible AI</span>
                  <span class="paper-venue-abbrv">(<a target="_blank" href="https://sites.google.com/view/rai-workshop/">RAI</a>)</span>, 2021
                  <img class="shields" src="https://img.shields.io/badge/-oral-darkred?style=flat-square" alt=""><br>
                </p> -->

                <!-- <div class="paper-buttons">
                  <a href="https://arxiv.org/abs/2102.12781" target="_blank" class="paper-button">arxiv</a>
 -->
                  <!-- change P{ID} and ABS{ID} -->
                  <!-- <a id="p4" class="paper-button abstract-button">abstract</a>
                  <a href="https://github.com/harshays/inputgradients"  target="_blank" class="paper-button">code</a>
                  <a href="https://slideslive.com/38955783"  target="_blank" class="paper-button">talk</a>
                </div> -->

                <!-- <div id="abs4" class="paper-abstract" style="display: none;">
                  <blockquote>
                    Interpretability methods that seek to explain instance-specific model predictions [Simonyan et al. 2014, Smilkov et al. 2017] are often based on the premise that the magnitude of input-gradient -- gradient of the loss with respect to input -- highlights discriminative features that are relevant for prediction over non-discriminative features that are irrelevant for prediction. In this work, we introduce an evaluation framework to study this hypothesis for benchmark image classification tasks, and make two surprising observations on CIFAR-10 and Imagenet-10 datasets: (a) contrary to conventional wisdom, input gradients of standard models (i.e., trained on the original data) actually highlight irrelevant features over relevant features; (b) however, input gradients of adversarially robust models (i.e., trained on adversarially perturbed data) starkly highlight relevant features over irrelevant features. To better understand input gradients, we introduce a synthetic testbed and theoretically justify our counter-intuitive empirical findings. Our observations motivate the need to formalize and verify common assumptions in interpretability, while our evaluation framework and synthetic dataset serve as a testbed to rigorously analyze instance-specific interpretability methods.
                  </blockquote>
                </div>
              </div>      -->

            </div>
          </div>
          <hr>
        </div>
        <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>
      </div>

      <!-- Papers -->
      <div id="paper" class="paper-row row">
        <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>

        <div class="col-lg-8 col-md-10 col-sm-12">
          <div class="subheader-row row">
            <div class="col-xs-12 subheader-col">
              <p class="subheader">Papers</p>
            </div>
          </div>
          <div class="papers-content-row row">
            <div class="col-xs-12 papers-content-col">

              <!-- Input Gradients -->
              <div class="paper-div">
                <a class="paper-title" target="_blank" href="https://arxiv.org/abs/2102.12781">Do Input Gradients Highlight Discriminative Features?</a>

                <p class="paper-info">
                  <span class="paper-authors"><span class="underline">Harshay Shah</span>, Prateek Jain, and Praneeth Netrapalli</span><br/>
                  <span class="paper-venue">ICLR workshop on Science and Engineering of Deep Learning</span>
                  <span class="paper-venue-abbrv">(<a target="_blank" href="https://sites.google.com/view/sedl-workshop">SEDL</a>)</span>, 2021
                  <img class="shields" src="https://img.shields.io/badge/-oral-darkred?style=flat-square" alt=""><br>
                  <span class="paper-venue">ICLR workshop on Responsible AI</span>
                  <span class="paper-venue-abbrv">(<a target="_blank" href="https://sites.google.com/view/rai-workshop/">RAI</a>)</span>, 2021
                  <img class="shields" src="https://img.shields.io/badge/-oral-darkred?style=flat-square" alt=""><br>
                </p>

                <div class="paper-buttons">
                  <a href="https://arxiv.org/abs/2102.12781" target="_blank" class="paper-button">arxiv</a>

                  <!-- change P{ID} and ABS{ID} -->
                  <a id="p4" class="paper-button abstract-button">abstract</a>
                  <a href="https://github.com/harshays/inputgradients"  target="_blank" class="paper-button">code</a>
                  <a href="https://slideslive.com/38955783"  target="_blank" class="paper-button">talk</a>
                </div>

                <div id="abs4" class="paper-abstract" style="display: none;">
                  <blockquote>
                    Interpretability methods that seek to explain instance-specific model predictions [Simonyan et al. 2014, Smilkov et al. 2017] are often based on the premise that the magnitude of input-gradient -- gradient of the loss with respect to input -- highlights discriminative features that are relevant for prediction over non-discriminative features that are irrelevant for prediction. In this work, we introduce an evaluation framework to study this hypothesis for benchmark image classification tasks, and make two surprising observations on CIFAR-10 and Imagenet-10 datasets: (a) contrary to conventional wisdom, input gradients of standard models (i.e., trained on the original data) actually highlight irrelevant features over relevant features; (b) however, input gradients of adversarially robust models (i.e., trained on adversarially perturbed data) starkly highlight relevant features over irrelevant features. To better understand input gradients, we introduce a synthetic testbed and theoretically justify our counter-intuitive empirical findings. Our observations motivate the need to formalize and verify common assumptions in interpretability, while our evaluation framework and synthetic dataset serve as a testbed to rigorously analyze instance-specific interpretability methods.
                  </blockquote>
                </div>
              </div>

              <!-- Simplicity Bias -->
              <div class="paper-div">
                <a class="paper-title" target="_blank" href="https://arxiv.org/abs/2006.07710v2">The Pitfalls of Simplicity Bias in Neural Networks</a>

                <p class="paper-info">
                  <span class="paper-authors"><span class="underline">Harshay Shah</span>, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli</span><br/>
                  <span class="paper-venue">Neural Information Processing Systems</span>
                  <span class="paper-venue-abbrv">(<a target="_blank" href="https://neurips.cc/Conferences/2020">NeurIPS</a>)</span>, 2020<br>
                  <span class="paper-venue">ICML workshop on Uncertainty and Robustness in Deep Learning</span>
                  <span class="paper-venue-abbrv">(<a target="_blank" href="https://sites.google.com/view/udlworkshop2020/home">UDL</a>)</span>, 2020<br>
                </p>

                <div class="paper-buttons">
                  <a href="https://arxiv.org/abs/2006.07710v2" target="_blank" class="paper-button">arxiv</a>

                  <!-- change P{ID} and ABS{ID} -->
                  <a id="p3" class="paper-button abstract-button">abstract</a>
                  <a href="https://drive.google.com/file/d/10McXcIyTM8pxJE2edqcvO2cBxmq8is2P/view?usp=sharing"  target="_blank" class="paper-button">poster</a>
                  <a href="https://github.com/harshays/simplicitybiaspitfalls"  target="_blank" class="paper-button">code</a>
                  <a href="https://slideslive.com/38935941"  target="_blank" class="paper-button">talk</a>
                </div>

                <div id="abs3" class="paper-abstract" style="display: none;">
                  <blockquote>
                    Several works have proposed Simplicity Bias (SB)&mdash;the tendency of standard training procedures such as Stochastic Gradient Descent (SGD) to find simple models&mdash;to justify why neural networks generalize well [Arpit et al. 2017, Nakkiran et al. 2019, Soudry et al. 2018]. However, the precise notion of simplicity remains vague. Furthermore, previous settings that use SB to theoretically justify why neural networks generalize well do not simultaneously capture the non-robustness of neural networks&mdash;a widely observed phenomenon in practice [Goodfellow et al. 2014, Jo and Bengio 2017]. We attempt to reconcile SB and the superior standard generalization of neural networks with the non-robustness observed in practice by designing datasets that (a) incorporate a precise notion of simplicity, (b) comprise multiple predictive features with varying levels of simplicity, and (c) capture the non-robustness of neural networks trained on real data. Through theory and empirics on these datasets, we make four observations: (i) SB of SGD and variants can be extreme: neural networks can exclusively rely on the simplest feature and remain invariant to all predictive complex features. (ii) The extreme aspect of SB could explain why seemingly benign distribution shifts and small adversarial perturbations significantly degrade model performance. (iii) Contrary to conventional wisdom, SB can also hurt generalization on the same data distribution, as SB persists even when the simplest feature has less predictive power than the more complex features. (iv) Common approaches to improve generalization and robustness&mdash;ensembles and adversarial training&mdash;can fail in mitigating SB and its pitfalls. Given the role of SB in training neural networks, we hope that the proposed datasets and methods serve as an effective testbed to evaluate novel algorithmic approaches aimed at avoiding the pitfalls of SB.
                  </blockquote>
                </div>
              </div>


              <!-- ARW -->
              <div class="paper-div">
                <a class="paper-title" target="_blank" href="https://arxiv.org/abs/1712.10195">Growing Attributed Networks through Local Processes</a>

                <p class="paper-info">
                  <span class="paper-authors"><span class="underline">Harshay Shah</span>, Suhansanu Kumar, and Hari Sundaram</span><br/>
                  <span class="paper-venue">World Wide Web Conference </span>
                  <span class="paper-venue-abbrv">(<a target="_blank" href="https://www2019.thewebconf.org/">WWW</a>)</span>, 2019
                </p>

                <div class="paper-buttons">
                  <a href="https://arxiv.org/abs/1712.10195" target="_blank" class="paper-button">arxiv</a>
                  <a id="p2" class="paper-button abstract-button">abstract</a>
                  <a href="https://drive.google.com/file/d/17S9guz_R7K9ulv3trLCIQQjQk6i4VOfm/view?usp=sharing"  target="_blank" class="paper-button">poster</a>
                  <a href="https://github.com/CrowdDynamicsLab/ARW"  target="_blank" class="paper-button">code</a>
                  <a href="https://crowddynamicslab.github.io/networks/2019/06/06/Growing-Attributed-Networks/"  target="_blank" class="paper-button">blog post</a>
                </div>

                <div id="abs2" class="paper-abstract" style="display: none;">
                  <blockquote>
                    This paper proposes an attributed network growth model. Despite the knowledge that individuals use limited resources to form connections to similar others, we lack an understanding of how local and resource-constrained mechanisms explain the emergence of rich structural properties found in real-world networks. We make three contributions. First, we propose a parsimonious and accurate model of attributed network growth that jointly explains the emergence of in-degree distributions, local clustering, clustering-degree relationship and attribute mixing patterns. Second, our model is based on biased random walks and uses local processes to form edges without recourse to global network information. Third, we account for multiple sociological phenomena: bounded rationality, structural constraints, triadic closure, attribute homophily, and preferential attachment. Our experiments indicate that the proposed Attributed Random Walk (ARW) model accurately preserves network structure and attribute mixing patterns of six real-world networks; it improves upon the performance of eight state-of-the-art models by a statistically significant margin of 2.5-10x.
                  </blockquote>
                </div>
              </div>

              <!-- Connected Components -->
              <div class="paper-div">
                <a class="paper-title" target="_blank" href="https://arxiv.org/abs/1812.00139">Number of Connected Components in a Graph: Estimation via Counting Patterns</a>

                <p class="paper-info">
                  <span class="paper-authors">Ashish Khetan, <span class="underline">Harshay Shah</span>, and Sewoong Oh</span><br/>
                  <span class="paper-venue">Preprint: arXiv:1812.00139</span>, 2018
                </p>

                <div class="paper-buttons">
                  <a href="https://arxiv.org/abs/1812.00139" target="_blank" class="paper-button">arxiv</a>
                  <a id="p1" class="paper-button abstract-button">abstract</a>
                  <a href="https://github.com/harshays/connectedcomponents"  target="_blank" class="paper-button">code</a>
                </div>

                <div id="abs1" class="paper-abstract" style="display: none;">
                  <blockquote>
                    Due to resource constraints and restricted access to large-scale graph datasets, it is often necessary to work with a sampled subgraph of a larger original graph. The task of inferring a global property of the original graph from the sampled subgraph is of fundamental interest. In this work, we focus on estimating the number of connected components. Due to the inherent difficulty of the problem for general graphs, little is known about the connection between the number of connected components in the observed subgraph and the original graph. To make this connection, we propose a highly redundant motif-based representation of the observed subgraph, which, at first glance, may seem counter-intuitive. However, the proposed representation is crucial in introducing a  novel estimator for the number of connected components in general graphs. The connection is made precise via the Schatten \(k\)-norms of the graph Laplacian and the spectral representation of the number of connected components.  We provide a guarantee on the resulting mean squared error that characterizes the bias-variance trade-off. Furthermore, our experiments on synthetic and real-world graphs show that we improve upon competing algorithms for graphs with spectral gaps bounded away from zero.
                  </blockquote>
                </div>
              </div>


            </div>
          </div>
          <hr>
        </div>

        <div class="col-lg-2 col-md-1 hidden-sm hidden-xs"></div>
      </div>
  </body>

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-55279868-1', 'auto');
    ga('send', 'pageview');
  </script>
</html>
